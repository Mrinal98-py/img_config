pip install tensorflow numpy opencv-python

##--

import tensorflow as tf
from tensorflow.keras import layers, models

def build_generator(input_shape=(256, 256, 3)):
    inputs = tf.keras.Input(input_shape)

    # Encoder
    conv1 = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)
    conv1 = layers.LeakyReLU(alpha=0.2)(conv1)
    
    conv2 = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(conv1)
    conv2 = layers.BatchNormalization()(conv2)
    conv2 = layers.LeakyReLU(alpha=0.2)(conv2)
    
    conv3 = layers.Conv2D(256, kernel_size=4, strides=2, padding='same')(conv2)
    conv3 = layers.BatchNormalization()(conv3)
    conv3 = layers.LeakyReLU(alpha=0.2)(conv3)
    
    conv4 = layers.Conv2D(512, kernel_size=4, strides=2, padding='same')(conv3)
    conv4 = layers.BatchNormalization()(conv4)
    conv4 = layers.LeakyReLU(alpha=0.2)(conv4)
    
    # Decoder
    deconv1 = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same')(conv4)
    deconv1 = layers.BatchNormalization()(deconv1)
    deconv1 = layers.ReLU()(deconv1)
    
    deconv2 = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(deconv1)
    deconv2 = layers.BatchNormalization()(deconv2)
    deconv2 = layers.ReLU()(deconv2)
    
    deconv3 = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(deconv2)
    deconv3 = layers.BatchNormalization()(deconv3)
    deconv3 = layers.ReLU()(deconv3)
    
    deconv4 = layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding='same')(deconv3)
    outputs = layers.Activation('tanh')(deconv4)

    return models.Model(inputs, outputs)

def build_discriminator(input_shape=(256, 256, 3)):
    inputs = tf.keras.Input(input_shape)

    conv1 = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)
    conv1 = layers.LeakyReLU(alpha=0.2)(conv1)
    
    conv2 = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(conv1)
    conv2 = layers.BatchNormalization()(conv2)
    conv2 = layers.Leaky


##--

def compile_gan(generator, discriminator):
    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    discriminator.trainable = False
    gan_input = tf.keras.Input(shape=(256, 256, 3))
    generated_image = generator(gan_input)
    gan_output = discriminator(generated_image)
    
    gan = models.Model(gan_input, gan_output)
    gan.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    return gan

def train_gan(generator, discriminator, gan, epochs, batch_size, image_dir, mask_dir):
    data_gen_args = dict(rescale=1./255)

    image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)
    mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)

    image_generator = image_datagen.flow_from_directory(
        image_dir,
        class_mode=None,
        target_size=(256, 256),
        batch_size=batch_size,
        seed=1)

    mask_generator = mask_datagen.flow_from_directory(
        mask_dir,
        class_mode=None,
        target_size=(256, 256),
        batch_size=batch_size,
        seed=1)

    train_generator = zip(image_generator, mask_generator)

    for epoch in range(epochs):
        for images, masks in train_generator:
            valid = np.ones((batch_size, 16, 16, 1))
            fake = np.zeros((batch_size, 16, 16, 1))
            
            generated_images = generator.predict(images)

            d_loss_real = discriminator.train_on_batch(masks, valid)
            d_loss_fake = discriminator.train_on_batch(generated_images, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
            
            g_loss = gan.train_on_batch(images, valid)
            
            print(f"Epoch: {epoch+1}, D Loss: {d_loss[0]}, G Loss: {g_loss[0]}")

    generator.save('gan_generator.h5')
    discriminator.save('gan_discriminator.h5')
    gan.save('gan_model.h5')

##--

def fix_broken_image(generator_model_path, image_path, output_path):
    generator = tf.keras.models.load_model(generator_model_path)
    
    image = cv2.imread(image_path)
    image_resized = cv2.resize(image, (256, 256))
    image_normalized = image_resized / 255.0
    image_input = np.expand_dims(image_normalized, axis=0)

    fixed_image = generator.predict(image_input)[0]
    fixed_image = (fixed_image * 255).astype(np.uint8)
    fixed_image_resized = cv2.resize(fixed_image, (image.shape[1], image.shape[0]))

    cv2.imwrite(output_path, fixed_image_resized)
    cv2.imshow('Fixed Image', fixed_image_resized)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# Path to the broken image
broken_image_path = 'path_to_broken_image.png'
output_image_path = 'path_to_fixed_image.png'

# Fix the broken image using the trained generator model
fix_broken_image('gan_generator.h5', broken_image_path, output_image_path)


